# Levenberg-Marquardt Algorithm

Levenberg-Marquardt Algorithm (LMA) 是一種用於解非線性最小二乘問題（Nonlinear Least Squares, NLS）的迭代方法，特別適用於非線性回歸、曲線擬合以及訓練神經網絡等情況。它結合了兩種經典的數值優化方法：梯度下降法（Gradient Descent）和高斯-牛頓法（Gauss-Newton Method），從而在收斂速度和穩定性之間取得了良好的平衡。

## 什麼是非線性最小化二乘問題

### 問題描述
假設我們有一組觀測數據 $(t_i, y_i)$，其中 $t_i$ 是自變量， $y_i$ 是應變量（即觀察值）。目標是找到一個非線性模型 $f(x, t)$，其中 $x$ 是待優化的參數向量，使得模型的預測值 $f(x, t_i)$ 與觀測值 $y_i$ 之間的誤差平方和最小化。這個優化問題可以表述為：

$$
\min_{\mathbf{x}} \sum_{i=1}^{n} \left( y_i - f(\mathbf{x}, t_i) \right)^2
$$

其中：

- $f(x, t_i)$ 是依賴於參數向量 $x$ 的非線性模型，輸入為自變量 $t_i$。
- $y_i$ 是觀察值。
- $x$ 是需要通過最小化誤差來估計的參數向量。

## LMA 的核心概念

### 1. 最小二乘法

最小二乘法的目的是找到參數，使得模型預測值與實際觀測值之間的平方誤差總和最小化。目標函數為：

$$
\min_{\mathbf{x}} \sum_{i=1}^{n} \left( y_i - f(\mathbf{x}, t_i) \right)^2
$$

其中， $y_i$ 是實際值， $f(x, t_i)$ 是模型的預測值， $x$ 是需要優化的參數向量， $t_i$ 是自變量。

### 2. 高斯-牛頓法

高斯-牛頓法是非線性最小二乘問題中一種高效的求解方法。它通過近似 Hessian 矩陣來進行優化：

$$
x_{k+1} = x_k - (J^T J)^{-1} J^T r
$$

其中， $J$ 是模型對參數的雅可比矩陣（Jacobian），$r$ 是殘差向量。高斯-牛頓法對於模型較接近解時收斂速度較快，但當解距離較遠時，這種方法可能不穩定。

### 3. 梯度下降法

梯度下降法通過計算誤差函數相對於參數的梯度，沿著梯度的反方向進行參數更新。這種方法相對穩定，但收斂速度較慢：

$$
\mathbf{x}_{k+1} = \mathbf{x}_k - \alpha \nabla E(\mathbf{x}_k)
$$

其中， $\alpha$ 是學習率， $E(x_k)$ 是誤差函數。

### 4. Levenberg-Marquardt 方法

Levenberg-Marquardt 方法結合了梯度下降和高斯-牛頓法。當解離初始估計較遠時，該算法更像梯度下降法；當接近解時，它更像高斯-牛頓法。更新公式為：

$$
x_{k+1} = x_k - (J^T J + \lambda I)^{-1} J^T r
$$

其中， $\lambda$ 是調整參數，它控制算法在高斯-牛頓法和梯度下降法之間的平衡。當 $\lambda$ 很大時，該方法類似於梯度下降法；當 $\lambda$ 很小時，該方法則接近高斯-牛頓法。

### 5. 調整參數 $\lambda$

在每次迭代中，$\lambda$ 的值根據誤差變化進行動態調整。如果新解使得誤差減少，則減小 $\lambda$；如果誤差增加，則增大 $\lambda$。這樣的調整使得算法在早期更穩定，接近解時收斂更快。

## Levenberg-Marquardt 方法的主要步驟

1. 初始化參數值，選擇初始參數向量 $x_0$ 和初始的 $\lambda$。
2. 計算雅可比矩陣 $J$，計算預測值與觀測值之間的誤差 $r$，並計算誤差的平方和。
3. 更新參數，使用 LMA 的公式來更新參數 $x$。
4. 動態調整 $\lambda$，根據誤差是否減少來調整 $\lambda$ 的值。
5. 迭代，重複步驟 2 至 4，直到誤差收斂或達到最大迭代次數。

## 優缺點

### 優點

- **高速收斂**：當解接近時，LMA 可以像高斯-牛頓法一樣快速收斂。
- **穩定性**：當解遠離時，LMA 提供了梯度下降的穩定性，避免數值問題。
- **適合小型問題**：由於它依賴雅可比矩陣和殘差的直接計算，LMA 在小型問題中非常高效。

### 缺點

- **計算量大**：每次迭代都需要計算雅可比矩陣的乘積並求逆，這在處理大規模數據或高維參數時會變得非常昂貴。
- **局部最優**：LMA 是一種局部優化算法，可能會陷入局部最優解，特別是當初始估計不佳時。
